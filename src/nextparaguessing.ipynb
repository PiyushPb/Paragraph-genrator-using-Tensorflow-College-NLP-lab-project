{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv('./data/fakenews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(text_df.text.values)\n",
    "joined_text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_text = joined_text[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(partial_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = np.unique(tokens)\n",
    "unique_token_index = {token: i for i, token in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 10\n",
    "input_words = []\n",
    "next_words = []\n",
    "\n",
    "for i in range(len(tokens) - n_words):\n",
    "    input_words.append(tokens[i:i + n_words])\n",
    "    next_words.append(tokens[i + n_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_words), n_words, len(unique_tokens)), dtype=bool)\n",
    "y = np.zeros((len(next_words), len(unique_tokens)), dtype=bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, words in enumerate(input_words):\n",
    "    for j, word in enumerate(words):\n",
    "        X[i, j, unique_token_index[word]] = 1\n",
    "    y[i, unique_token_index[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(n_words, len(unique_tokens)), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(unique_tokens)))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "135/135 [==============================] - 35s 226ms/step - loss: 7.0662 - accuracy: 0.0569\n",
      "Epoch 2/40\n",
      "135/135 [==============================] - 31s 229ms/step - loss: 6.7185 - accuracy: 0.0575\n",
      "Epoch 3/40\n",
      "135/135 [==============================] - 28s 207ms/step - loss: 6.6994 - accuracy: 0.0575\n",
      "Epoch 4/40\n",
      "135/135 [==============================] - 26s 195ms/step - loss: 6.6862 - accuracy: 0.0575\n",
      "Epoch 5/40\n",
      "135/135 [==============================] - 31s 227ms/step - loss: 6.6748 - accuracy: 0.0575\n",
      "Epoch 6/40\n",
      "135/135 [==============================] - 30s 222ms/step - loss: 6.6639 - accuracy: 0.0575\n",
      "Epoch 7/40\n",
      "135/135 [==============================] - 25s 189ms/step - loss: 6.6508 - accuracy: 0.0575\n",
      "Epoch 8/40\n",
      "135/135 [==============================] - 24s 175ms/step - loss: 6.6401 - accuracy: 0.0575\n",
      "Epoch 9/40\n",
      "135/135 [==============================] - 24s 174ms/step - loss: 6.6282 - accuracy: 0.0576\n",
      "Epoch 10/40\n",
      "135/135 [==============================] - 24s 174ms/step - loss: 6.6140 - accuracy: 0.0576\n",
      "Epoch 11/40\n",
      "135/135 [==============================] - 23s 173ms/step - loss: 6.5877 - accuracy: 0.0576\n",
      "Epoch 12/40\n",
      "135/135 [==============================] - 23s 174ms/step - loss: 6.5513 - accuracy: 0.0576\n",
      "Epoch 13/40\n",
      "135/135 [==============================] - 23s 174ms/step - loss: 6.5141 - accuracy: 0.0582\n",
      "Epoch 14/40\n",
      "135/135 [==============================] - 24s 181ms/step - loss: 6.4785 - accuracy: 0.0577\n",
      "Epoch 15/40\n",
      "135/135 [==============================] - 27s 201ms/step - loss: 6.4463 - accuracy: 0.0579\n",
      "Epoch 16/40\n",
      "135/135 [==============================] - 27s 203ms/step - loss: 6.4185 - accuracy: 0.0596\n",
      "Epoch 17/40\n",
      "135/135 [==============================] - 27s 200ms/step - loss: 6.3801 - accuracy: 0.0626\n",
      "Epoch 18/40\n",
      "135/135 [==============================] - 26s 193ms/step - loss: 6.3443 - accuracy: 0.0658\n",
      "Epoch 19/40\n",
      "135/135 [==============================] - 24s 174ms/step - loss: 6.3032 - accuracy: 0.0674\n",
      "Epoch 20/40\n",
      "135/135 [==============================] - 23s 174ms/step - loss: 6.2556 - accuracy: 0.0702\n",
      "Epoch 21/40\n",
      "135/135 [==============================] - 23s 173ms/step - loss: 6.2081 - accuracy: 0.0710\n",
      "Epoch 22/40\n",
      "135/135 [==============================] - 23s 174ms/step - loss: 6.1563 - accuracy: 0.0723\n",
      "Epoch 23/40\n",
      "135/135 [==============================] - 23s 173ms/step - loss: 6.1082 - accuracy: 0.0756\n",
      "Epoch 24/40\n",
      "135/135 [==============================] - 23s 174ms/step - loss: 6.0596 - accuracy: 0.0786\n",
      "Epoch 25/40\n",
      "135/135 [==============================] - 26s 195ms/step - loss: 6.0101 - accuracy: 0.0792\n",
      "Epoch 26/40\n",
      "135/135 [==============================] - 34s 255ms/step - loss: 5.9562 - accuracy: 0.0819\n",
      "Epoch 27/40\n",
      "135/135 [==============================] - 34s 248ms/step - loss: 5.9109 - accuracy: 0.0841\n",
      "Epoch 28/40\n",
      "135/135 [==============================] - 36s 268ms/step - loss: 5.8684 - accuracy: 0.0859\n",
      "Epoch 29/40\n",
      "135/135 [==============================] - 34s 256ms/step - loss: 5.8188 - accuracy: 0.0907\n",
      "Epoch 30/40\n",
      "135/135 [==============================] - 34s 252ms/step - loss: 5.7748 - accuracy: 0.0927\n",
      "Epoch 31/40\n",
      "135/135 [==============================] - 31s 232ms/step - loss: 5.7304 - accuracy: 0.0958\n",
      "Epoch 32/40\n",
      "135/135 [==============================] - 29s 214ms/step - loss: 5.6846 - accuracy: 0.0972\n",
      "Epoch 33/40\n",
      "135/135 [==============================] - 31s 231ms/step - loss: 5.6317 - accuracy: 0.1025\n",
      "Epoch 34/40\n",
      "135/135 [==============================] - 29s 213ms/step - loss: 5.5852 - accuracy: 0.1048\n",
      "Epoch 35/40\n",
      "135/135 [==============================] - 27s 199ms/step - loss: 5.5383 - accuracy: 0.1102\n",
      "Epoch 36/40\n",
      "135/135 [==============================] - 29s 217ms/step - loss: 5.4905 - accuracy: 0.1138\n",
      "Epoch 37/40\n",
      "135/135 [==============================] - 27s 202ms/step - loss: 5.4426 - accuracy: 0.1182\n",
      "Epoch 38/40\n",
      "135/135 [==============================] - 27s 200ms/step - loss: 5.3937 - accuracy: 0.1250\n",
      "Epoch 39/40\n",
      "135/135 [==============================] - 30s 223ms/step - loss: 5.3401 - accuracy: 0.1296\n",
      "Epoch 40/40\n",
      "135/135 [==============================] - 30s 221ms/step - loss: 5.2892 - accuracy: 0.1326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13b35a84940>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01), metrics=['accuracy'])\n",
    "model.fit(X,y, batch_size=128, epochs=40, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\final year projects\\Para genrator - clg project\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/nextparaguessing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/nextparaguessing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_words(input_text, n_best):\n",
    "    input_text = input_text.lower()\n",
    "    X = np.zeros((1, n_words, len(unique_tokens)))\n",
    "    for i, word in enumerate(input_text.split()):\n",
    "        X[0, i, unique_token_index[word]] = 1\n",
    "    \n",
    "    predictions = model.predict(X)[0]\n",
    "    return np.argpartition(predictions, n_best)[:n_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 890ms/step\n"
     ]
    }
   ],
   "source": [
    "possible = predict_next_words('The president of United states of america announced that he', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sounding', 'brand', 'chanted', 'blamed', 'fundamentalism']\n"
     ]
    }
   ],
   "source": [
    "print([unique_tokens[i] for i in possible])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrate_text(input_text, text_length, creativity = 3):\n",
    "    word_sequence = input_text.lower().split()\n",
    "    current = 0\n",
    "    for _ in range(text_length):\n",
    "        sub_sequence = \" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower())[current:current+n_words])\n",
    "        try:\n",
    "            choise = unique_tokens[random.choice(predict_next_words(sub_sequence, creativity))]\n",
    "        except:\n",
    "            choise = random.choice(unique_tokens)\n",
    "        word_sequence.append(choise)\n",
    "        current += 1\n",
    "    return \" \".join(word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the president of united states of america announced that he chanted i thousands deal deal country va history own pandering paris t navy united deal country deal fbi party united brand deal deal country va number group m m va fbi own reprinted series clinton men t deal va country state history brand images thousands deal weapon hatch threshold frail fbi provider hatch country va fbi house television group deal history deal history fbi community united deal deal deal fbi deal deal deal deal country own nationalism m group own nationalism united party country national paris united 94 country separate own country whopping i m in his clinton final meme'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genrate_text(\"The president of United states of america announced that he\", 100, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Para genrator - clg project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
