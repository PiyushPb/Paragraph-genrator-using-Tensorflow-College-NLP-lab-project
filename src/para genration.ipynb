{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = tf.keras.utils.get_file('shakespeare.txt',\n",
    "        'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text = open(filepath, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data #\n",
    "The problem that we have right now with our data is that we are dealing with text. We cannot just train a neural network on letters or sentences. We need to convert all of these values into numerical data. So we have to come up with a system that allows us to convert the text into numbers, then predict specific numbers based on that data and then again convert the resulting numbers back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity I am going to modify the last code line that we wrote. In this case I immediately convert all of the text into lower-case so that we have fewer possible choices. Also I am not going to use the whole text file as training data. If you have the capacities or the time to train your model on the whole data, do it! It will produce much better results. But if your machine is slow or you have limited time, you might consider just using a part of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[300000:800000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select all the characters from character number 300,000 up until 800,000. So we are processing a total of 500,000 characters, which should be enough for pretty descent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(set(text))\n",
    "\n",
    "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a sorted set of all the unique characters that occur in the text. In a set no value appears more than once, so this is a good way to filter out the characters. After that we define two structures for converting the values. Both are dictionaries that enumerate the characters. In the first one, the characters are the keys and the indices are the values. In the second one it is the other way around. Now we can easily convert a character into a unique numerical representation and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 40\n",
    "STEP_SIZE = 3\n",
    "\n",
    "sentences = []\n",
    "next_char = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next step, we define how long a sequence shall be and also how many characters we will step further to start the next sentence. What we try to do here is to take sentences and then save the next letter as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
    "    sentences.append(text[i: i + SEQ_LENGTH])\n",
    "    next_char.append(text[i + SEQ_LENGTH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate through the whole text and gather all sentences and their next character. This is the training data for our neural network. Now we just need to convert it into a numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.zeros((len(sentences), SEQ_LENGTH, len(characters)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(characters)), dtype=bool)\n",
    "\n",
    "for i, satz in enumerate(sentences):\n",
    "    for t, char in enumerate(satz):\n",
    "        x[i, t, char_to_index[char]] = True\n",
    "    y[i, char_to_index[next_char[i]]] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Recurrent Neural Network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Activation, Dense, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can also just refer to these things manually if you want to. We will use Sequential for our model, Activation, Dense and LSTM for our layers and RMSprop for optimization during the compilation of our model. LSTM stands for long-short-term memory and is a type of recurrent neural network layer. It might be called the memory of our model. This is crucial, since we are dealing with sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128,\n",
    "               input_shape=(SEQ_LENGTH,\n",
    "                            len(characters))))\n",
    "model.add(Dense(len(characters)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our structure is simple! The inputs immediately flow into our LSTM layer with 128 neurons. Our input shape is the length of a sentence times the amount of characters. The character which shall follow will be set to True or one. This layer is followed by a Dense hidden layer, which just increases complexity. In the end we use the Softmax activation function in order to make our results add up to one. This gives us the probability for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "651/651 [==============================] - 72s 108ms/step - loss: 2.7156\n",
      "Epoch 2/4\n",
      "651/651 [==============================] - 66s 101ms/step - loss: 2.3234\n",
      "Epoch 3/4\n",
      "651/651 [==============================] - 65s 99ms/step - loss: 2.2068\n",
      "Epoch 4/4\n",
      "651/651 [==============================] - 62s 95ms/step - loss: 2.1235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d96904c4c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=0.01))\n",
    "\n",
    "model.fit(x, y, batch_size=256, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compile the model and train it with our training data that we prepared above. We choose a batch size of 256 (which you can change if you want) and four epochs. This means that our model is going to see the same data four times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function #\n",
    "\n",
    "Our model is now trained but it only outputs the probabilities for the next character. We therefore need some additional functions to make our script generate some reasonable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text #\n",
    "Now we can get to the final function of our script. The function that generates the final text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, temperature):\n",
    "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "        x_predictions = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_predictions[0, t, char_to_index[char]] = 1\n",
    "\n",
    "        predictions = model.predict(x_predictions, verbose=0)[0]\n",
    "        next_index = sample(predictions,\n",
    "                                 temperature)\n",
    "        next_character = index_to_char[next_index]\n",
    "\n",
    "        generated += next_character\n",
    "        sentence = sentence[1:] + next_character\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is less complicated than it looks. We basically choose a random starting position within the text because we need some starting text in order to predict the “next” character. So basically the first SEQ_LENGTH amount of characters will be copied from the original text. But we could just cut them off afterwards and we would end up with text that is completely generated by our neural network.\n",
    "\n",
    "So we choose some random starting text and then we run a for loop in the range of the length that we want. We can generate a text with 100 characters or one with 20,000. We then convert our sentence into the desired input format that we already talked about. The sentence is now an array with ones or Trues, wherever a character occurs. Then we use the predict method of our model, to predict the likelihoods of the next characters. Then we make use of our sample helper function. In this function we also have a temperature parameter, which we can pass to that helper function. Of course the result we get needs to be converted from the numerical format into a readable character. Once this is done, we add the character to our generated text and repeat the process, until we reach the desired length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results #\n",
    "The results are actually quite good! Let’s take a look at some samples. I played around with the parameters, in order to diversify the results. I am not going to post the full results here but just some interesting snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ine own again, be gone,\n",
      "that i might stre with the the the sord the garenoun the sore the the be the rous the sarent the mand the the cound with the sear the seand the sore the to the sores corenter seath me the beand the sour not the mare the cours the sing the seand sour be the surest the sore the sore the ment the sore the beast the so\n",
      "ear her hence.\n",
      "\n",
      "queen margaret:\n",
      "so come my sing thy bears woll my seall the merend so for will,\n",
      "in the buck shat the se preast the with the bewer ar and gead in the of my to butered she the nowes my the fore for the coust the sing the sand the but the sand she gord am the goond the ward the songer.\n",
      "\n",
      "cours:\n",
      "and ser:\n",
      "wher and be wath at she\n",
      "roat of death.\n",
      "rescue, fair lord, or else the to mard to the gores:\n",
      "en the sord mome to loms on the the hare roule dore the beath farle my to which rey and or she she congers.\n",
      "\n",
      "lick of lile::\n",
      "ce all thes shell wor mun the my wars deand the hane or thy if of the geallont\n",
      "you soul the and you to nomen she for sing\n",
      "courser whes she dur the l\n",
      "xenes:\n",
      "my best camillo! we must disguise the to gor thoug ad the the hing laver co nond for mes and she trou the came thee shengerens for and nor ment thou hine shish recathe uno dat ir povene to my sourland our pests.\n",
      "\n",
      "herrell:\n",
      "ad i wor to ghe stoul ond king you courst me souk\n",
      "the routh ar our for hou hat se prick no ceant herr usd the g\n",
      "his face:\n",
      "as brittle as the glory is the kise and and late for the stoull;\n",
      "and sive thet the cotprench pi han the castt!\n",
      "ack seer dothou shor to me sary shand hand?\n",
      "\n",
      "her, macheri:\n",
      "no sto piis thee contennig, no de't sow seathis hat the cuth weronde that in mane;\n",
      "buth king thee with be jowing thit sell in of munt you soue'r what then mo, t\n",
      "ry's oath and your succession.\n",
      "short tall wis ly unes of ou mamen daged.\n",
      "\n",
      "hing: to mule badleft out sigh wow kisp gist:\n",
      "and the inge ty tat shar kny ors sthath nenra,\n",
      "a dich now br woll flonk cwibled hat your, lor and hine rave ta soure\n",
      "war tou it the best'e th tilg shall the loue trene, so cather, of he nithhall onc that this nougfark\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(300, 0.2))\n",
    "print(generate_text(300, 0.4))\n",
    "print(generate_text(300, 0.5))\n",
    "print(generate_text(300, 0.6))\n",
    "print(generate_text(300, 0.7))\n",
    "print(generate_text(300, 0.8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Para genrator - clg project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
